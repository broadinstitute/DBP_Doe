# HTC Submit File

# Provide HTCondor with the name of the Docker container
docker_image = suganyasivaguru/dbpdoe:v1
universe = docker

#scripts to execute 
executable = executable.sh

#creating arguments that can be used from the csv file 
arguments = $(epoch) $(model_name) $(batch_size) $(optimizer) $(learning_rate)

#to have one directory for all the output files from one job 
#intialdir = job_$(ClusterID)

log = job_$(ClusterID)_$(ProcID).log
error = job_$(ClusterID)_$(ProcID).err
output = job_$(ClusterID)_$(ProcID).out

#files to transfer 
transfer_input_files = /home/sivagurunath/project/DBP_Doe/data/sourcedata,\
		       /home/sivagurunath/project/DBP_Doe/data/targetdata,\
		       /home/sivagurunath/project/DBP_Doe/data/testsource/18_C1_all.tif,\
		       /home/sivagurunath/project/DBP_Doe/data/testtarget/18_C1_all_masks.tif,\
                       /home/sivagurunath/project/DBP_Doe/data/predictdata/070721_Slide2_Animal1_all.tif,\
		       /home/sivagurunath/project/DBP_Doe/FromScratch.py
                       
                       

#preserve_relative_paths = True 

# Transferring files out on exit
should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# Requirments to get access to COBA server
accounting_group = COBA_BroadCimini
requirements = (Machine == "coba2000.chtc.wisc.edu") && (OpSysMajorVer == 7 || OpSysMajorVer == 8)
			
#resource
request_cpus = 1
request_memory = 20GB
request_disk = 20GB
request_gpus = 1

queue epoch,model_name,batch_size,optimizer,learning_rate from variables_1.txt
